{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2170465,"sourceType":"datasetVersion","datasetId":1165452}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***A Large-Scale Dataset for Segmentation and Classification***\n\nAuthors: O. Ulucan, D. Karakaya, M. Turkan\nDepartment of Electrical and Electronics Engineering, Izmir University of Economics, Izmir, Turkey\nCorresponding author: M. Turkan\nContact Information: mehmet.turkan@ieu.edu.tr\n\n\n***General Introduction***\n\nThis dataset contains 9 different seafood types collected from a supermarket in Izmir, Turkey\nfor a university-industry collaboration project at Izmir University of Economics, and this work\nwas published in ASYU 2020.\nDataset includes, gilt head bream, red sea bream, sea bass, red mullet, horse mackerel, \nblack sea sprat, striped red mullet, trout, shrimp image samples. \n\nIf you use this dataset in your work, please consider to cite:\n\n@inproceedings{ulucan2020large,\n  title={A Large-Scale Dataset for Fish Segmentation and Classification},\n  author={Ulucan, Oguzhan and Karakaya, Diclehan and Turkan, Mehmet},\n  booktitle={2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},\n  pages={1--5},\n  year={2020},\n  organization={IEEE}\n}\n\n* O.Ulucan , D.Karakaya and M.Turkan.(2020) A large-scale dataset for fish segmentation and classification.\nIn Conf. Innovations Intell. Syst. Appli. (ASYU)\n\n***Purpose of the work***\n\nThis dataset was collected in order to carry out segmentation, feature extraction and classification tasks\nand compare the common segmentation, feature extraction and classification algortihms (Semantic Segmentation, Convolutional Neural Networks, Bag of Features).\nAll of the experiment results prove the usability of our dataset for purposes mentioned above.\n\n\n***Data Gathering Equipment and Data Augmentation***\n\nImages were collected via 2 different cameras, Kodak Easyshare Z650 and Samsung ST60. \nTherefore, the resolution of the images are 2832 x 2128, 1024 x 768, respectively.\n\nBefore the segmentation, feature extraction and classification process, the dataset was resized to 590 x 445\nby preserving the aspect ratio. After resizing the images, all labels in the dataset were augmented (by flipping and rotating). \n\nAt the end of the augmentation process, the number of total images for each class became 2000; 1000 for the RGB fish images\nand 1000 for their pair-wise ground truth labels. \n\n\n***Description of the data in this data set***\n\nThe dataset contains 9 different seafood types. For each class, there are 1000 augmented images and their pair-waise augmented ground truths.\nEach class can be found in the \"Fish_Dataset\" file with their ground truth labels. All images for each class are ordered from \"00000.png\" to \"01000.png\".\n \nFor example, if you want to access the ground truth images of the shrimp in the dataset, the order should be followed is \"Fish->Shrimp->Shrimp GT\". ","metadata":{}},{"cell_type":"code","source":"# !pip install gradio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\nimport logging\nfrom tqdm import tqdm\nfrom itertools import compress\nfrom typing import Optional, List, Tuple\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom PIL import Image\nimport cv2 as cv\nfrom scipy.cluster.vq import kmeans2\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.applications.resnet import ResNet50,preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom tensorflow.keras.saving import load_model\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Layer, Flatten\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam ,RMSprop\nfrom tensorflow.keras import saving\n\nimport gradio as gr\n\n\npd.set_option(\"display.max_columns\", None)\npd.set_option('display.max_colwidth', None)\nplt.style.use(\"ggplot\")\nsns.set_palette(sns.diverging_palette(220, 20))\n%load_ext tensorboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Explatory Data Analysis","metadata":{}},{"cell_type":"code","source":"fish_dataset_directory =\"/kaggle/input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_image_classes(images_path: str) -> List[str]:\n    \"\"\"\n    Find subdirectory names in the specified directory.\n\n    Parameters\n    ----------\n    images_path : str\n        Path to the directory containing subdirectories.\n\n    Returns\n    -------\n    List[str]\n        A list of subdirectory names found within the specified directory.\n\n    Example\n    -------\n    classes = find_image_classes(\"path/to/fish_dataset_directory\")\n    print(classes)\n    \"\"\"\n    return [i for i in os.listdir(images_path) if os.path.isdir(os.path.join(images_path, i))]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_classes = find_image_classes(fish_dataset_directory)\nimage_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_from_image_folders(images_path: str, extension: Optional[str] = \"png\") -> pd.DataFrame:\n    \"\"\"\n    Create a DataFrame from image files in specified directories.\n\n    Parameters\n    ----------\n    images_path : str\n        Path to the directory containing subdirectories of images.\n    extension : str, optional\n        The file extension of the images to include (default is \"png\").\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame with two columns: 'path' containing file paths and 'label' containing the class labels.\n\n    Notes\n    -----\n    Excludes directories that contain 'GT' in their names.\n\n    Example\n    -------\n    df = df_from_image_folders(\"path/to/fish_dataset_directory\")\n    print(df.head())\n    \"\"\"\n    \n    label = []\n    path = []\n    image_files = glob.glob(os.path.join(images_path, \"**\", f\"*.{extension.lower()}\"), recursive=True)\n\n    for file in image_files:\n        dirpath = os.path.dirname(file)\n        folder_name = os.path.basename(dirpath)\n        if \"GT\" not in folder_name:\n            label.append(folder_name)\n            path.append(file)\n\n    class_dict = {\"path\": path, \"label\": label}\n    return pd.DataFrame(class_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_from_image_folders(fish_dataset_directory)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_fish_from_each_class(df: pd.DataFrame, img_size: Tuple[int, int] = (224, 224)) -> None:\n    \"\"\"\n    Displays one image from each unique class in the DataFrame.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing image paths and labels.\n    img_size : Tuple[int, int]\n        Size to which images will be resized for display.\n    \"\"\"\n    \n    plt.figure(figsize=(12, 12))\n    \n    for i, unique_label in enumerate(df[\"label\"].unique()):\n        \n        plt.subplot(3, 3, i + 1)\n        image_path = df[df[\"label\"] == unique_label].iloc[0, 0]\n        img = load_img(image_path, target_size=img_size)\n        plt.imshow(img)\n        plt.title(unique_label)\n        plt.axis('off')\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_fish_from_each_class(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_images_from_class(df: pd.DataFrame, class_name: str, num_images: int, img_size: Tuple[int, int] = (224, 224)) -> None:\n    \"\"\"\n    Displays a specified number of images from a given class in the DataFrame.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing image paths and labels.\n    class_name : str\n        The class label to filter the images.\n    img_size : Tuple[int, int]\n        Size to which images will be resized for display.\n    num_images : int\n        Number of images to display from the given class.\n    \"\"\"\n    images = df[df[\"label\"] == class_name][\"path\"].iloc[:num_images]\n    plt.figure(figsize=(12, 12))\n\n    for i, image_path in enumerate(images):\n        plt.subplot(4, 4, i + 1)\n        plt.imshow(load_img(image_path, target_size=img_size))\n        plt.title(image_path[-5:-4]) \n        plt.axis('off')\n\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images_from_class(df, \"Sea Bass\", 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Görüntülerin boyutunu, renk kanallarını ve dağılımlarını analiz edilmesi","metadata":{}},{"cell_type":"code","source":"df[\"label\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"label\"]].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"label\", data = df )\nplt.xticks(rotation = 90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Boyut ve renk kanalları analizi yapılarak ortalama genişlik ve yükseklik değerleri belirlenip, yeniden boyutlandırma için uygun genişlik ve yükseklik değerleri seçilmiştir.**","metadata":{}},{"cell_type":"code","source":"\ndef compute_image_statistics_from_df(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes average width, height, and channel count for images listed in the DataFrame.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing 'path' and 'label' columns for images.\n\n    Returns:\n        pd.DataFrame: DataFrame containing average statistics for each fish class.\n    \"\"\"\n    stats = []\n\n    grouped = df.groupby('label')\n\n    for label, group in grouped:\n        widths = []\n        heights = []\n        channel_counts = []\n\n        for _, row in group.iterrows():\n            image_path = row['path']\n            try:\n                image = load_img(image_path)\n                image_array = img_to_array(image)\n                \n                width, height = image.size\n                widths.append(width)\n                heights.append(height)\n                channel_counts.append(image_array.shape[2])\n            except Exception as e:\n                print(f\"Error loading image {image_path}: {e}\")\n        \n        if widths:  \n            avg_width = np.mean(widths)\n            avg_height = np.mean(heights)\n            avg_channels = np.mean(channel_counts)\n            min_width = np.min(widths)\n            max_width = np.max(widths)\n            min_height = np.min(height)\n            max_height = np.max(height)\n\n            stats.append({\n                'Fish Class': label,\n                'Average Width': avg_width,\n                'Average Height': avg_height,\n                'Average Channels': avg_channels,\n                'Min Width': min_width,\n                'Max Width' : max_width,\n                'Min Height' : min_height,\n                'Max Height' : max_height  \n            })\n\n    return pd.DataFrame(stats)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_statistics = compute_image_statistics_from_df(df)\ndf_statistics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_rgb_channels(image_path: str) -> None:\n    \"\"\"\n    Displays the individual RGB channels of an image.\n\n    Parameters\n    ----------\n    image_path : str\n        The file path to the image.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    image = Image.open(image_path)\n    \n    r, g, b = image.split()\n    \n    r_array = np.array(r)\n    g_array = np.array(g)\n    b_array = np.array(b)\n\n    fig, axes = plt.subplots(1, 3, figsize=(12,12))\n\n    axes[0].imshow(r_array, cmap=\"Reds\")\n    axes[0].set_title(\"Red Channel\")\n    axes[0].axis(\"off\")\n\n    axes[1].imshow(g_array, cmap=\"Greens\")\n    axes[1].set_title(\"Green Channel\")\n    axes[1].axis(\"off\")\n\n    axes[2].imshow(b_array, cmap=\"Blues\")\n    axes[2].set_title(\"Blue Channel\")\n    axes[2].axis(\"off\")\n\n\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Örnek bir resmin renk kanallarının görselleştirilmesi**","metadata":{}},{"cell_type":"code","source":"sample_image = \"/kaggle/input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset/Sea Bass/Sea Bass/00026.png\"\ndisplay_rgb_channels(sample_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Performing Preprocessing Steps on Images\n\n- **Since there are a large number of images in the dataset, preprocessing steps have been carried out to filter these images in the best possible way.**\n\n- **Cropping, embedding, clustering, splitting, normalization, and data augmentation processes have been applied.**\n\n- **Since all images are of the same size, a min-max resolution filter has not been applied. If these images had been scraped from the internet with different sizes, all images could have been resized to the same dimensions, and those below the threshold could have been removed from the dataset.**\n","metadata":{}},{"cell_type":"markdown","source":"**Image Cropping Demo**\n\n**This section aims to show how smart cropping works. Smart cropping, removes the unneccessary parts of the image and focuses the main object of the image. For this purpose all images converted to PIL image** \n","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(\n    level=logging.INFO, format= \"%(asctime)s - %(levelname)s - %(message)s\"\n)\n\n\ndef load_image(image_path: str) -> Image.Image:\n    \"\"\"\n    Load an image in RGB format from the given path.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to the image file.\n\n    Returns\n    -------\n    Image.Image\n        Loaded image.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    return image\n\ndef load_images_from_df(df: pd.DataFrame) -> List[Image.Image]:\n    \"\"\"\n    Load images from a DataFrame containing image paths.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame with a column \"path\" for image file paths.\n\n    Returns\n    -------\n    List[Image.Image]\n        List of loaded images.\n    \"\"\"\n    images = [load_image(image_path) for image_path in tqdm(df[\"path\"].values.tolist(), total=len(df))]\n    logging.info(f\"Loaded {len(images)} images from DataFrame\")\n    return images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_images = load_images_from_df(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image Cropping**","metadata":{}},{"cell_type":"code","source":"def plot_image(image: Image.Image) -> None:\n    \"\"\"\n    Display an image using matplotlib with the axis turned off.\n    \n    Parameters\n    ----------\n    image : Image.Image\n        The image to be displayed.\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_image(loaded_images[20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Smart Crop Script**","metadata":{}},{"cell_type":"code","source":"\nEXTENSIONS = (\"jpg\", \"JPG\", \"jpeg\", \"JPEG\", \"png\", \"PNG\")\n\ndef detect(pil_image: Image.Image, square: bool = False) -> np.ndarray:\n    \"\"\"\n    Detect keypoints in the image using SIFT and apply cropping.\n\n    Parameters\n    ----------\n    pil_image : Image.Image\n        The input image to detect keypoints and crop.\n    square : bool, optional\n        If True, the output image will be square-shaped. Defaults to False.\n    \n    Returns\n    -------\n    np.ndarray\n        The cropped image as a numpy array.\n    \"\"\"\n    img = np.array(pil_image) \n    gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)  \n    sift = cv.SIFT_create(edgeThreshold=8)\n    kp = sift.detect(gray, None)\n\n    all_points = [i.pt for i in kp]\n    x_points = [z[0] for z in all_points]\n    y_points = [z[1] for z in all_points]\n    thresh = 0\n    x_min, y_min = int(min(x_points)) - thresh, int(min(y_points) - thresh)\n    x_max, y_max = int(max(x_points)) + thresh, int(max(y_points) + thresh)\n    min_side = min((x_max - x_min), (y_max - y_min))\n    max_side = max((x_max - x_min), (y_max - y_min))\n    x_mean, y_mean = int((x_max + x_min) / 2), int((y_max + y_min) / 2)\n\n    squared_x_min, squared_x_max = x_mean - int(min_side / 2), x_mean + int(min_side / 2)\n    squared_y_min, squared_y_max = y_mean - int(min_side / 2), y_mean + int(min_side / 2)\n\n    if not square:\n        return img[y_min:y_max, x_min:x_max] \n\n    return img[squared_y_min:squared_y_max, squared_x_min:squared_x_max] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cropped_image = detect(loaded_images[20], square=True)\nplot_image(cropped_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image Preprocessing and Saving to Disk**","metadata":{}},{"cell_type":"markdown","source":"Helper Functions","metadata":{}},{"cell_type":"code","source":"def process_image(image: Image.Image, square: bool = True) -> Image.Image:\n    \"\"\"\n    Apply smart cropping to the image using SIFT keypoints detection.\n\n    Parameters\n    ----------\n    image : Image.Image\n        The image to be processed and cropped.\n    square : bool, optional\n        If True, the output image will be square-shaped. Defaults to True.\n    \n    Returns\n    -------\n    Image.Image\n        Cropped image based on detected keypoints.\n    \"\"\"\n    cropped_image = detect(image, square=square)\n    return Image.fromarray(cropped_image)\n\ndef create_directory(dir_path: str) -> None:\n    \"\"\"\n    Create a directory if it doesn't exist.\n    \n    Parameters\n    ----------\n    dir_path : str\n        The path of the directory to create.\n    \"\"\"\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n        logging.info(f\"Directory created: {dir_path}\")\n    else:\n        logging.info(f\"Directory already exists: {dir_path}\")\n        \n\ndef save_image(image: Image.Image, save_path: str) -> None:\n    \"\"\"\n    Save a PIL image to a specific path.\n    \n    Parameters\n    ----------\n    image : Image.Image\n        The image to save.\n    save_path : str\n        The path where the image should be saved.\n    \"\"\"\n    image.save(save_path)\n    logging.info(f\"Saved image to {save_path}\")\n\n\ndef process_and_save_images(df: pd.DataFrame, output_base_dir: str) -> None:\n    \"\"\"\n    Load, process, and save images organized by labels into respective directories.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing 'path' and 'label' columns for images and their labels.\n    output_base_dir : str\n        The base directory where images will be saved, organized by labels.\n    \"\"\"\n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        image_path = row['path']\n        label = row['label']\n        \n        image = load_image(image_path)\n        \n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image) \n        if not isinstance(image, Image.Image):\n            raise ValueError(\"Input image must be a numpy array or PIL image\")\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\") \n        \n        processed_image = process_image(image, square=True)\n        \n        label_dir = os.path.join(output_base_dir, label)\n        create_directory(label_dir)\n        \n        image_name = os.path.basename(image_path) \n        save_path = os.path.join(label_dir, image_name) \n        \n        save_image(processed_image, save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_path = \"/kaggle/working/preprocessed_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_and_save_images(df, output_path )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Control**","metadata":{}},{"cell_type":"code","source":"preprocessed_images = \"/kaggle/working/preprocessed_images\"\ndf_preprocessed = df_from_image_folders(preprocessed_images)\ndf_preprocessed.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preprocessed[\"label\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_fish_from_each_class(df_preprocessed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image Clustering Process**\n\n- **Unwanted images that are not suitable for the model may be mixed in the data. Since we cannot manually check and separate a large number of images one by one, we can classify the images using unsupervised learning techniques to more easily remove these images from the dataset.**\n\n- **To do this, we first need to extract the embeddings of the images. The ResNet50 model has been used for embedding extraction.**\n\n- **The final softmax layer of the ResNet50 model has been removed, and the embeddings from the fully connected layer with 2048 units have been obtained. That is, the values of z = W.A + b, which pass through the activation function.**\n\n- **Since the size of the embeddings vectors; is 2048, it is necessary to reduce the dimensions of these vectors. The more we reduce the dimensions, the more suitable the data becomes for clustering.**\n\n- **PCA has been used for the dimensionality reduction process.**\n\n- **It is necessary to perform standardization before PCA and K-means. However, since our data already came from ResNet in a certain standard, no additional standardization has been performed.**\n\n- **The clustering process has been successful. The fish images have been classified according to the direction of the photo capture. However, images that will not be used in model training have not been detected as a result of the classification.**\n","metadata":{}},{"cell_type":"code","source":"class Img2VecKeras:\n    def __init__(self):\n        \"\"\"\n        Initialize the Img2VecKeras class with the ResNet50 model for embedding extraction.\n        \"\"\"\n        base_model = ResNet50(weights='imagenet')\n        self.model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n    \n    def get_vec(self, img_paths: List[str]) -> np.ndarray:\n        \"\"\"\n        Extract embeddings for the given list of image paths.\n\n        Parameters\n        ----------\n        img_paths : List[str]\n            A list of paths to the images for which embeddings are to be extracted.\n        \n        Returns\n        -------\n        np.ndarray\n            An array of shape (n_samples, n_features) containing the embeddings.\n        \"\"\"\n        embeddings = []\n        for img_path in tqdm(img_paths, desc=\"Extracting Embeddings\", unit=\"image\"):\n            img = load_image(img_path) \n            img = img.resize((224, 224)) \n            img = image.img_to_array(img)  \n            img = np.expand_dims(img, axis=0)  \n            img = preprocess_input(img)  \n            embedding = self.model.predict(img, verbose=0)  \n            embeddings.append(embedding.flatten()) \n        return np.array(embeddings)\n\ndef extract_and_save_embeddings_from_df(df: pd.DataFrame, embedding_path: str) -> None:\n    \"\"\"\n    Extract embeddings from images using a DataFrame and save them to CSV files.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing image paths and their respective labels.\n    embedding_path : str\n        The directory where the extracted embeddings will be saved as CSV files.\n    \n    Returns\n    -------\n    None\n    \"\"\"\n   \n    if not os.path.exists(embedding_path):\n        os.makedirs(embedding_path)\n\n    grouped = df.groupby(\"label\")\n\n    for label, group in grouped:\n        img_paths = group['path'].tolist()  \n        \n        img2vec_keras = Img2VecKeras()\n        embeddings = img2vec_keras.get_vec(img_paths)\n\n        df_embeddings = pd.DataFrame(embeddings)\n        df_embeddings[\"filepaths\"] = img_paths\n        \n        output_file = os.path.join(embedding_path, f\"{label}_embeddings.csv\")\n        df_embeddings.to_csv(output_file, index=False)\n        print(f\"Saved embeddings for {label} to {output_file}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extract Embeddings**","metadata":{}},{"cell_type":"code","source":"embedding_path = \"/kaggle/working/preprocessed_images_embeddings\"\nextract_and_save_embeddings_from_df(df_preprocessed, embedding_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_embeddings_from_directory(directory: str) -> List[Tuple[np.ndarray, List[str]]]:\n    \"\"\"\n    Load embeddings from all CSV files in the specified directory.\n\n    Parameters\n    ----------\n    directory : str\n        Path to the directory containing the CSV files.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, List[str]]]\n        A list of tuples, each containing embeddings and corresponding file paths from a CSV file.\n    \"\"\"\n    all_embeddings = []\n    for file in os.listdir(directory):\n        if file.endswith(\".csv\"):\n            file_path = os.path.join(directory, file)\n            embeddings = pd.read_csv(file_path)\n            file_paths = embeddings[\"filepaths\"].tolist()\n            embeddings = embeddings.drop(\"filepaths\", axis=1).values\n            all_embeddings.append((embeddings, file_paths))\n    return all_embeddings\n\ndef calculate_pca(embeddings: np.ndarray, dim: int = 16) -> np.ndarray:\n    \"\"\"\n    Calculate PCA for the given embeddings.\n\n    Parameters\n    ----------\n    embeddings : np.ndarray\n        The input embeddings to be reduced.\n    dim : int\n        The number of dimensions to reduce to.\n\n    Returns\n    -------\n    np.ndarray\n        The PCA-reduced embeddings.\n    \"\"\"\n    print(\"Calculating PCA\")\n    pca = PCA(n_components=dim)\n    pca_embeddings = pca.fit_transform(embeddings)\n    print(\"PCA calculation done!\")\n    return pca_embeddings\n\ndef calculate_kmeans(embeddings: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform KMeans clustering on the embeddings.\n\n    Parameters\n    ----------\n    embeddings : np.ndarray\n        The input embeddings to be clustered.\n    k : int\n        The number of clusters.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the centroids and the labels assigned to each embedding.\n    \"\"\"\n    print(\"KMeans processing...\")\n    centroids, labels = kmeans2(data=embeddings, k=k, minit=\"points\")\n    print(\"KMeans done!\")\n    return centroids, labels\n\ndef create_dir(directory: str) -> None:\n    \"\"\"\n    Create a directory if it does not exist.\n\n    Parameters\n    ----------\n    directory : str\n        The path to the directory to create.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\ndef copy_images_to_clusters(image_paths: List[str], labels: np.ndarray, cluster_range: int, class_name: str) -> None:\n    \"\"\"\n    Copy images to their corresponding cluster directories.\n\n    Parameters\n    ----------\n    image_paths : List[str]\n        The list of image paths.\n    labels : np.ndarray\n        The labels assigned to each image by the clustering algorithm.\n    cluster_range : int\n        The number of clusters.\n    class_name : str\n        The name of the class for the output directory structure.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    for label_number in tqdm(range(cluster_range)):\n        label_mask = labels == label_number\n        path_images = list(compress(image_paths, label_mask))\n        target_directory = f\"clusters/{class_name}/cluster_{label_number}\"\n        create_dir(target_directory)\n        for image_path in path_images:\n            shutil.copy2(image_path, target_directory)\n\ndef run_clustering(embeddings_dir: str, pca_dim: int = 16, cluster_range: int = 4) -> None:\n    \"\"\"\n    Run PCA and KMeans clustering on embeddings in all CSV files within the specified directory\n    and copy clustered images to respective directories.\n\n    Parameters\n    ----------\n    embeddings_dir : str\n        Path to the directory containing the embeddings CSV files.\n    pca_dim : int\n        The dimension for PCA reduction.\n    cluster_range : int\n        The number of clusters for KMeans.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    all_embeddings = load_embeddings_from_directory(embeddings_dir)\n\n    for index, (embeddings, image_paths) in enumerate(all_embeddings):\n        class_name = os.path.basename(os.listdir(embeddings_dir)[index]).replace('.csv', '')\n        pca_embeddings = calculate_pca(embeddings=embeddings, dim=pca_dim)\n        centroids, labels = calculate_kmeans(pca_embeddings, k=cluster_range)\n        copy_images_to_clusters(image_paths, labels, cluster_range, class_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clustering**","metadata":{}},{"cell_type":"code","source":"embeddings_path = \"/kaggle/working/preprocessed_images_embeddings\"\nrun_clustering(embeddings_path, pca_dim=16, cluster_range=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef show_images(image_paths: List[str], title: str) -> None:\n    \"\"\"\n    Display images from the provided list of image paths.\n\n    Parameters\n    ----------\n    image_paths : List[str]\n        A list of paths to the images to be displayed.\n    title : str\n        The title for the displayed images.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # Assuming you have some image displaying logic here, e.g., using matplotlib\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(15, 5))\n    plt.suptitle(title, fontsize=20)\n\n    for i, image_path in enumerate(image_paths):\n        img = load_img(image_path)\n        plt.subplot(1, len(image_paths), i + 1)\n        plt.imshow(img)\n        plt.axis('off')\n\n    plt.show()\n\ndef show_cluster_images(cluster_dir: str, cluster_count: int = 4, images_to_show: int = 5) -> None:\n    \"\"\"\n    Display images from each cluster directory.\n\n    Parameters\n    ----------\n    cluster_dir : str\n        The base directory containing the cluster directories.\n    cluster_count : int\n        The number of clusters to display images from.\n    images_to_show : int\n        The number of images to display from each cluster.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    for i in range(cluster_count):\n        cluster_cat_dir = os.path.join(cluster_dir, f'cluster_{i}')\n        cluster_cat_files = os.listdir(cluster_cat_dir)\n        cluster_images_to_show_cat = [os.path.join(cluster_cat_dir, img_file) for img_file in cluster_cat_files[:images_to_show]]\n        show_images(cluster_images_to_show_cat, f'Cluster {i}')\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Control Cluster Images**","metadata":{}},{"cell_type":"code","source":"show_cluster_images('/kaggle/working/clusters/Red Sea Bream_embeddings', cluster_count=4, images_to_show=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Creating the Deep Learning (ANN) Model\n\n### 3.1. Splitting the Fish_Dataset into Train, Validation, and Test Sets\n- **To use for validation during model training, 10% of the dataset has been set aside as the validation set. After the model training is completed, a test set has been created to evaluate the performance.**\n","metadata":{}},{"cell_type":"code","source":"def copy_images(subset_df: pd.DataFrame, target_dir: str) -> None:\n    \"\"\"\n    Copies images from the DataFrame to the specified target directory.\n\n    Parameters:\n    - subset_df (pd.DataFrame): A DataFrame containing image paths in the 'path' column.\n    - target_dir (str): The target directory where the images will be copied.\n\n    Returns:\n    - None: This function does not return any value.\n    \"\"\"\n    for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=f\"Copying images to {target_dir}\"):\n        shutil.copy2(row['path'], target_dir)\n\n\ndef split_dataset_by_count(\n    df_processed: pd.DataFrame, \n    output_dir: str, \n    train_ratio: float = 0.8, \n    val_ratio: float = 0.1, \n    test_ratio: float = 0.1\n) -> pd.DataFrame:\n    \"\"\"\n    Splits dataset images into train, validation, and test sets based on fixed ratio for each class.\n\n    Parameters:\n    - df_processed (pd.DataFrame): DataFrame containing 'path' and 'label' columns for images.\n    - output_dir (str): Directory where the train/validation/test folders will be created.\n    - train_ratio (float): Proportion of data to allocate to the train set.\n    - val_ratio (float): Proportion of data to allocate to the validation set.\n    - test_ratio (float): Proportion of data to allocate to the test set.\n\n    Returns:\n    - pd.DataFrame: A DataFrame summarizing the counts of images in each set (train/val/test) per class.\n    \"\"\"\n    \n  \n    df_processed = shuffle(df_processed).reset_index(drop=True)\n    class_summary = {'Class': [], 'Train Count': [], 'Validation Count': [], 'Test Count': []}\n    \n   \n    for class_name, class_group in df_processed.groupby('label'):\n        \n        total_images = len(class_group)\n        train_count = int(total_images * train_ratio)\n        val_count = int(total_images * val_ratio)\n        test_count = total_images - train_count - val_count \n\n        class_summary['Class'].append(class_name)\n        class_summary['Train Count'].append(train_count)\n        class_summary['Validation Count'].append(val_count)\n        class_summary['Test Count'].append(test_count)\n\n        train_df = class_group[:train_count]\n        val_df = class_group[train_count:train_count + val_count]\n        test_df = class_group[train_count + val_count:]\n        \n        train_dir = os.path.join(output_dir, 'train', class_name)\n        val_dir = os.path.join(output_dir, 'validation', class_name)\n        test_dir = os.path.join(output_dir, 'test', class_name)\n        \n        for dir_path in [train_dir, val_dir, test_dir]:\n            if not os.path.exists(dir_path):\n                os.makedirs(dir_path)\n        \n        copy_images(train_df, train_dir)\n        copy_images(val_df, val_dir)\n        copy_images(test_df, test_dir)\n    \n    summary_df = pd.DataFrame(class_summary)\n    \n    return summary_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_images_output = \"/kaggle/working/model_dataset\"\nsummary_df = split_data_into_train_val_test(df_preprocessed, model_images_output)\n\nsummary_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(data=summary_df, x =\"Set\", y= \"Count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Data augmentation\n\n- **Normalization**: Rescaling pixel values from the range of 0-255 to 0-1.\n- **Random Rotation**: Rotating images up to 40 degrees randomly.\n- **Random Width and Height Shift**: Shifting images randomly by 20% of their width and height.\n- **Random Shear**: Applying random shearing transformations with a shear intensity of 0.2.\n- **Random Zoom**: Zooming in on images randomly by 20%.\n- **Horizontal Flip**: Flipping images horizontally.\n- **Fill Method**: Using the 'nearest' fill mode for newly created pixels during transformations.","metadata":{}},{"cell_type":"code","source":"train_dir = '/kaggle/working/model_dataset/train'  \nvalidation_dir = '/kaggle/working/model_dataset/validation'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale=1./255,          \n    rotation_range=40,       \n    width_shift_range=0.2,    \n    height_shift_range=0.2,\n    shear_range=0.2,          \n    zoom_range=0.2,           \n    horizontal_flip=True,     \n    fill_mode='nearest'   \n)\n\n\nval_datagen = ImageDataGenerator(rescale=1./255) \n\n# load train images\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(150, 150),  # resizing\n    batch_size=32,\n    class_mode='binary'      # binary classification (cat, dog)\n)\n\n# load validation images\nvalidation_generator = val_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(150, 150),  \n    batch_size=32,\n    class_mode='binary'   \n)\n\nclass_indices = train_generator.class_indices\nprint(\"Class labels:\", class_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data augmentation Control**","metadata":{}},{"cell_type":"code","source":"batch = next(train_generator)\nimages, labels = batch\nnum_images = min(5, len(images))\n\nplt.figure(figsize=(10, 10))\nfor i in range(num_images):\n    plt.subplot(1, 5, i + 1)\n    plt.imshow(images[i]) \n    plt.title(f\"Label: {int(labels[i])}\")  \n    plt.axis('off') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3. Model Selection and Evaluation\n\n**The following operations have been performed:**\n\n- **Selection of the deep learning model.**\n- **Using the training and validation datasets to train the selected model. Dropout and batch normalization are applied to prevent overfitting.**\n- **Determining the necessary hyperparameters.**\n\n**The deep learning architecture will utilize ANN (Artificial Neural Network). To achieve better results, CNN layers or pre-trained models like ResNet or EfficientNet can be fine-tuned using transfer learning methods. In this case study, an ANN model has been created.**\n\n**Using Class Structure with Keras Functional API**\n- The Functional API offers the ability to customize model architecture in a more detailed and flexible way. This is especially useful when creating more complex and customized network structures.\n  \n- More complex models may require features like multiple inputs and outputs, shared layers, or skip connections. The Functional API provides the necessary flexibility to manage such architectures.\n  \n- Using the Functional API in model design allows for gaining experience in working with more complex structures. This helps develop a habit for building more advanced network architectures in the future. This way, one can master a broader range of models without being limited to the Sequential API.\n\n","metadata":{}},{"cell_type":"code","source":"class CustomANN(Model):\n    def __init__(self, input_shape=(150, 150, 3), num_classes=9, name=\"custom_ann\", **kwargs):\n        super(CustomANN, self).__init__(name=name, **kwargs)\n        self.num_classes = num_classes\n\n        # Flatten layer to convert 2D images to 1D\n        self.flatten = Flatten()\n\n        # Fully connected layers\n        self.fc1 = Dense(512, activation='relu')\n        self.bn1 = BatchNormalization()  # Batch normalization after first dense layer\n        self.drop1 = Dropout(0.5)\n        \n        self.fc2 = Dense(256, activation='relu')\n        self.bn2 = BatchNormalization()  # Batch normalization after second dense layer\n        self.drop2 = Dropout(0.5)\n\n        self.fc3 = Dense(num_classes, activation='softmax')  # Output layer for multi-class classification\n\n    def call(self, inputs, training=False):\n        x = self.flatten(inputs)\n        x = self.fc1(x)\n        x = self.bn1(x, training=training)  # Apply batch normalization\n        x = self.drop1(x, training=training)\n        \n        x = self.fc2(x)\n        x = self.bn2(x, training=training)  # Apply batch normalization\n        x = self.drop2(x, training=training)\n        \n        outputs = self.fc3(x)  # Output layer\n        return outputs\n\n    def get_config(self):\n        config = super(CustomANN, self).get_config()\n        config.update({\n            \"input_shape\": (150, 150, 3),\n            \"num_classes\": self.num_classes\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n    def train(self, train_dir, val_dir, batch_size=32, epochs=10):\n        # Data preprocessing\n        train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            rotation_range=40,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True,\n            fill_mode='nearest'\n        )\n        \n        validation_datagen = ImageDataGenerator(rescale=1./255)\n        \n        train_generator = train_datagen.flow_from_directory(\n            train_dir,\n            target_size=(150, 150),\n            batch_size=batch_size,\n            class_mode='categorical'  # Categorical for multi-class\n        )\n        \n        validation_generator = validation_datagen.flow_from_directory(\n            val_dir,\n            target_size=(150, 150),\n            batch_size=batch_size,\n            class_mode='categorical'  # Categorical for multi-class \n        )\n        \n        # ModelCheckpoint callback\n        checkpoint_callback = ModelCheckpoint(\n            filepath='/kaggle/working/models/model_checkpoint_{epoch:02d}.keras',  \n            monitor='val_loss',\n            save_best_only=True,  \n            save_weights_only=False,\n            mode='min',\n            verbose=1\n        )\n\n        # EarlyStopping callback\n        early_stopping_callback = EarlyStopping(\n            monitor='val_loss',\n            patience=5,  # Stop training after 5 epochs without improvement\n            verbose=1\n        )\n        \n        # TensorBoard callback\n        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n        \n        # Define learning rate\n        optimizer = Adam(learning_rate=0.001)\n        \n        self.compile(optimizer=optimizer,\n                     loss='categorical_crossentropy',  # Categorical loss for multi-class classification\n                     metrics=['accuracy'])\n        \n        # Train the model\n        history = self.fit(\n            train_generator,\n            epochs=epochs,\n            validation_data=validation_generator,\n            callbacks=[checkpoint_callback, tensorboard_callback, early_stopping_callback],\n        )\n        \n        return history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = \"/kaggle/working/model_dataset/train\"\nval_dir = \"/kaggle/working/model_dataset/validation\"\n\nmodel = CustomANN(input_shape=(150, 150, 3), num_classes=9)\nmodel.train(train_dir=train_dir, val_dir=val_dir, batch_size=32, epochs=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if os.path.exists(\"/kaggle/working/model_dataset\"):\n#     shutil.rmtree(\"/kaggle/working/model_dataset\")  # Klasörü ve içindeki her şeyi sil\n#     print(f\"{output_path} has been deleted.\")\n# else:\n#     print(f\"{output_path} does not exist.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}